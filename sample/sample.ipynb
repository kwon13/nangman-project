{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 입력 확인\n",
    "- 입력 단어: 안녕하세요\n",
    "- 입력 정의: '편한 사이에서, 서로 만나거나 헤어질 때 정답게 하는 인사말.', '친한 사이에서 서로 만나거나 헤어질 때 인사로 하는 말.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"klue/roberta-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids(['[SEP]'])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids(['[CLS]'])[0]\n",
    "pad_id = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
    "unk_id = tokenizer.convert_tokens_to_ids(['[UNK]'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "definition = []\n",
    "ins = {'definition':['편한 사이에서, 서로 만나거나 헤어질 때 정답게 하는 인사말.', '친한 사이에서 서로 만나거나 헤어질 때 인사로 하는 말.']}\n",
    "\n",
    "for word in ins['definition']:\n",
    "    definition.extend(tokenizer.tokenize(word))\n",
    "definition = tokenizer.convert_tokens_to_ids(definition)\n",
    "\n",
    "input = [cls_id] + [mask_id] * k + [sep_id] + definition\n",
    "\n",
    "input = input[:256] \n",
    "input.append(sep_id) # input 최대 길이 = 255([CLS], [], [MASK]*k, ) + 1('[SEP]') = 256\n",
    "ins['input'] = input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2bpes = []\n",
    "word2idx = {}\n",
    "word = '안녕하세요'\n",
    "\n",
    "\n",
    "if word not in word2idx:\n",
    "    word2idx[word] = len(word2idx)\n",
    "    bpes = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    word2bpes.append(bpes)\n",
    "\n",
    "number_word_in_train = len(word2idx)\n",
    "\n",
    "\n",
    "if word not in word2idx:\n",
    "    word2idx[word] = len(word2idx)\n",
    "    bpes = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "    word2bpes.append(bpes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'안녕하세요': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5891, 2205, 5971]]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word2idx)\n",
    "word2bpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'train'\n",
    "max_word_len = 5\n",
    "\n",
    "for idx in range(1):\n",
    "    if unk_id in bpes:\n",
    "        if name == 'train':\n",
    "            continue # bpes = target word 토큰\n",
    "        else:\n",
    "            bpes = [0] * (max_word_len + 1)  # +1이 있는 이유는 모르겠음. 아마 이후 if문에서 train을 거르기 위해서?\n",
    "    if len(bpes) <= max_word_len:\n",
    "        ins['target'] = idx\n",
    "        \n",
    "    else:\n",
    "        if name != 'train':\n",
    "            ins['target'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전:  [5891, 2205, 5971]\n",
      "후:  [5891, 2205, 5971, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "max_word_len = 5\n",
    "for i in range(len(word2bpes)):\n",
    "        bpes = word2bpes[i] # bpes = target word 토큰\n",
    "        print('전: ', bpes)\n",
    "        bpes = bpes[:max_word_len] + [mask_id] * max(0, max_word_len - len(bpes))\n",
    "        print('후: ', bpes)\n",
    "        word2bpes[i] = bpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5891, 2205, 5971, 4, 4]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2bpes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "class RDRobertaForMaskedLM(RobertaForMaskedLM):\n",
    "    def set_start_end(self, start=1, end=5):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        masked_lm_labels=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        lm_labels=None,\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output[:, self.start:self.end])\n",
    "\n",
    "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # (ltr_lm_loss), (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENRobertaReverseDict(nn.Module):\n",
    "    def __init__(self, pre_name, word2bpes, pad_id, number_word_in_train):\n",
    "        super().__init__()\n",
    "        self.roberta_model = RDRobertaForMaskedLM.from_pretrained(pre_name)\n",
    "        self.roberta_model.set_start_end(1, 1+len(word2bpes[0]))\n",
    "        self.max_word_len = len(word2bpes[0])\n",
    "        # 1 x 1 x vocab_size\n",
    "        word2bpes = torch.LongTensor(word2bpes).transpose(0, 1).unsqueeze(0) \n",
    "        self.register_buffer('word2bpes', word2bpes)\n",
    "        self.number_word_in_train = number_word_in_train\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input 형식: cls + mask + sep_id + definition\n",
    "        \"\"\"\n",
    "        attention_mask = input.ne(self.pad_id)\n",
    "\n",
    "        #  batch_size x max_len x vocab_size\n",
    "        bpe_reps = self.roberta_model(input_ids=input, token_type_ids=None,\n",
    "                                                        attention_mask=attention_mask)[0]\n",
    "\n",
    "        # bsz x max_word_len x word_vocab_size\n",
    "        word2bpes = self.word2bpes.repeat(bpe_reps.size(0), 1, 1)\n",
    "        word_scores = bpe_reps.gather(dim=-1, index=word2bpes)   # bsz x max_word_len x word_vocab_size\n",
    "\n",
    "        word_scores = word_scores.sum(dim=1)\n",
    "        if self.training and self.number_word_in_train is not None:\n",
    "            word_scores = word_scores[:, :self.number_word_in_train]\n",
    "\n",
    "        return {'pred': word_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model = ENRobertaReverseDict(\"klue/roberta-small\", word2bpes, pad_id=pad_id,\n",
    "                          number_word_in_train=1)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-67df5955cdbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbpe_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "attention_mask = [pad_id]*len(input)\n",
    "bpe_reps = model(input_ids=input, token_type_ids=None, attention_mask=attention_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
