{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046dc12-8976-4f6a-83ce-6e9e8b53369a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8c9c893f-ce78-4c91-84d3-a98d1364da04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | bert_mlm | BertForMaskedLM | 110 M \n",
      "---------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.604   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: accelerator=ddp_spawn and num_workers=0 may result in data loading bottlenecks. Consider setting num_workers>0 and persistent_workers=True\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: num_workers>0, persistent_workers=False, and accelerator=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|                                           | 0/47 [00:00<?, ?it/s][W reducer.cpp:1263] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1263] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  85%|▊| 40/47 [00:08<00:01,  4.46it/s, loss=1.06, v_num=2, val_loss=9.1\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 47/47 [00:09<00:00,  5.18it/s, loss=1.06, v_num=2, val_loss=0.8\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 1:  83%|▊| 39/47 [00:07<00:01,  4.94it/s, loss=0.143, v_num=2, val_loss=0.^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 62, in <module>\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "    main()\n",
      "  File \"train.py\", line 58, in main\n",
      "    trainer.fit(model)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\n",
      "    self._run(model)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\n",
      "    self.dispatch()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: cleaning up ddp environment...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 122, in start_training\n",
      "    mp.spawn(self.new_process, **self.mp_spawn_kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py\", line 109, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --k 5 --max_epochs 10 --batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ee299854-2133-40de-8cc0-17061bce82d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "### desc: 죄와 벌- 도스토옙스키,살해한다)를 살해하고 돈을 훔치며 방황하다 결국 사랑과 신 앞에 자신의 과오를 뉘우치고 새삶을 찾는다는 다분히 개과천선적인 이야기이다. 하지만 이 소설을 내내 편히 읽으면서 자연스럽게 파악하게 되는 것이 있는데 바로 라스콜리니코프와 대척점에... ###\n",
      "0: ('개과천선', 0.999996542930603)\n",
      "1: ('수어지교', 3.1489112188864965e-06)\n",
      "2: ('함흥차사', 1.2230759693920845e-07)\n",
      "3: ('군계일학', 5.1923993993341355e-08)\n",
      "4: ('백전백승', 4.173730161483036e-08)\n",
      "5: ('조령모개', 3.407961912671453e-08)\n",
      "6: ('마부작침', 2.771530560607971e-08)\n",
      "7: ('독서망양', 1.782189684718105e-08)\n",
      "8: ('대기만성', 6.924367745853033e-09)\n",
      "9: ('다다익선', 3.5056400005117894e-09)\n"
     ]
    }
   ],
   "source": [
    "!python3 infer.py --desc '죄와 벌- 도스토옙스키,살해한다)를 살해하고 돈을 훔치며 방황하다 결국 사랑과 신 앞에 자신의 과오를 뉘우치고 새삶을 찾는다는 다분히 개과천선적인 이야기이다. 하지만 이 소설을 내내 편히 읽으면서 자연스럽게 파악하게 되는 것이 있는데 바로 라스콜리니코프와 대척점에...' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44765f",
   "metadata": {},
   "source": [
    "### model arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b41b63-c360-4080-9cca-496b2dd57f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "string = \"왜 응급실만 들어가면 함흥차사인지, 빨리 진행이 잘 될 줄 알고 응급실을 갔는데 이건 오히려 왜 더 걸리는지 답답하셨던 분들을 위해 응급실의 입원까지의 과정에 대해 간략히 적어보았다. 하지만 이는 일반적인 상황이며 정말 초응급한 상황에서는 모든 것이 정신없이 지나간다.\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"klue/roberta-small\", output_hidden_states =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d5c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "lefts = [\" \".join([\"[MASK]\"] * 5)]\n",
    "rights = [string]\n",
    "encodings = tokenizer(text=lefts,\n",
    "                              text_pair=rights,\n",
    "                              return_tensors=\"pt\",\n",
    "                              add_special_tokens=True,\n",
    "                              truncation=True,\n",
    "                              padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40dc926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_subword = model.forward(**encodings).logits\n",
    "S_subword = S_subword[:, 1:5+1]\n",
    "S_subword.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a535da4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_subword.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18e7880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[29867,  2232,  2063,     4,     4],\n",
       "        [29647,  2333,  2540,     4,     4],\n",
       "        [ 6133,  2087,  2221,     4,     4],\n",
       "        [  617,  2418,  2210,  2218,     4],\n",
       "        [ 5889,  2154,  2047,     4,     4],\n",
       "        [ 1295,  4379,  2120,     4,     4],\n",
       "        [  558,  2145,  2337,  2020,     4],\n",
       "        [ 1552,  2291,  2391,  2019,     4],\n",
       "        [15871,  2427,  2020,     4,     4],\n",
       "        [ 1132,  2165,  2353,  2164,     4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['함흥차사', '마부작침', '독서망양', '군계일학', '대기만성', '수어지교', '개과천선', '조령모개', '다다익선', '백전백승']\n",
    "mask_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "pad_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "encoded = tokenizer(text=vocab,\n",
    "                    add_special_tokens=False,\n",
    "                    padding='max_length',\n",
    "                    max_length=5,  # set to k\n",
    "                    return_tensors=\"pt\")\n",
    "input_ids = encoded['input_ids']\n",
    "input_ids[input_ids == pad_id] = mask_id  # replace them with masks\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8667e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2subs= input_ids\n",
    "word2subs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7454aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2subs = word2subs.T.repeat(S_subword.shape[0], 1, 1)  # (|V|, K) -> (N, K, |V|)\n",
    "word2subs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a807d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1390, -4.3971,  1.1603,  0.8054,  2.6666,  3.3189,  2.9078,\n",
       "           4.2063, -1.5453,  2.6153],\n",
       "         [ 3.0248, -1.3976, -0.9941,  1.9440,  2.8471, -4.1056,  3.5755,\n",
       "          -2.2962, -3.0727,  3.3772],\n",
       "         [ 3.4585,  2.1410,  2.1075,  3.9764,  2.3541,  1.3856,  0.6869,\n",
       "           2.5727,  2.9207,  0.9953],\n",
       "         [-4.2341, -4.2341, -4.2341,  3.3528, -4.2341, -4.2341,  3.4979,\n",
       "           3.1271, -4.2341,  0.4532],\n",
       "         [-3.9272, -3.9272, -3.9272, -3.9272, -3.9272, -3.9272, -3.9272,\n",
       "          -3.9272, -3.9272, -3.9272]]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_subword.gather(dim=-1, index=word2subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cc942d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.8171, -11.8150,  -5.8877,   6.1515,  -0.2935,  -7.5625,   6.7409,\n",
       "           3.6828,  -9.8587,   3.5137]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_subword = S_subword.gather(dim=-1, index=word2subs).sum(dim=1)\n",
    "S_subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a90cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "y = torch.tensor([2]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d979b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.1246, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "F.cross_entropy(S_subword, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526041be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
